{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c0638f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+---------+------------+--------+-------+\n",
      "|order_id| customer_name | product | order_date | amount | region|\n",
      "+--------+---------------+---------+------------+--------+-------+\n",
      "|       1|         Alice |  Laptop | 2023-01-15 |  55000 |  North|\n",
      "|       2|            Bob|   Mobile|  15-02-2023|   30000|  South|\n",
      "|       3|         Carol |  Tablet | 2023/03/10 |   NULL |   East|\n",
      "|       4|          David|   Laptop| 2023-13-01 |  65000 |   West|\n",
      "|       5|           Eve |  Mobile |  2023-04-05|  ERROR |  North|\n",
      "|       6|          Frank|   Laptop| 05-05-2023 |   72000|  South|\n",
      "|       7|          Grace|   Tablet|  2023-06-18|   48000|   East|\n",
      "|       7|          Grace|   Tablet|  2023-06-18|   48000|   East|\n",
      "|       8|         Heidi |  Mobile |            |  50000 |   West|\n",
      "|       9|           Ivan|   Laptop|  2023-07-22|  85,000|  North|\n",
      "|      10|          Judy |  Tablet | 2023-08-30 |        |  South|\n",
      "+--------+---------------+---------+------------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark=SparkSession.builder\\\n",
    "    .appName(\"CleaningSalesData\")\\\n",
    "        .getOrCreate()\n",
    "#read csv file sales4.csv\n",
    "raw_df=spark.read.format(\"csv\")\\\n",
    "    .option(\"inferschema\",\"true\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .load(\"../input/sales4.csv\")\n",
    "raw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb36832b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n",
      "+--------+-------------+-------+----------+-------+------+\n",
      "|order_id|customer_name|product|order_date| amount|region|\n",
      "+--------+-------------+-------+----------+-------+------+\n",
      "|       1|        Alice| Laptop|2023-01-15|55000.0| North|\n",
      "|       2|          Bob| Mobile|15-02-2023|30000.0| South|\n",
      "|       3|        Carol| Tablet|2023/03/10|   NULL|  East|\n",
      "|       4|        David| Laptop|2023-13-01|65000.0|  West|\n",
      "|       5|          Eve| Mobile|2023-04-05|   NULL| North|\n",
      "|       6|        Frank| Laptop|05-05-2023|72000.0| South|\n",
      "|       7|        Grace| Tablet|2023-06-18|48000.0|  East|\n",
      "|       7|        Grace| Tablet|2023-06-18|48000.0|  East|\n",
      "|       8|        Heidi| Mobile|          |50000.0|  West|\n",
      "|       9|         Ivan| Laptop|2023-07-22|85000.0| North|\n",
      "|      10|         Judy| Tablet|2023-08-30|   NULL| South|\n",
      "+--------+-------------+-------+----------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#clean column names (trim, lower)\n",
    "col_trim_df=raw_df.toDF(*[c.strip().lower() for c in raw_df.columns]) # all columns names are already in lower case\n",
    "\n",
    "#Trim all string columns\n",
    "trim_df=col_trim_df.withColumn(\"order_id\",trim(col_trim_df[\"order_id\"]))\\\n",
    "    .withColumn(\"customer_name\",trim(col_trim_df[\"customer_name\"]))\\\n",
    "        .withColumn(\"product\",trim(col_trim_df[\"product\"]))\\\n",
    "            .withColumn(\"order_date\",trim(col_trim_df[\"order_date\"]))\\\n",
    "                .withColumn(\"amount\",trim(col_trim_df[\"amount\"]))\\\n",
    "                    .withColumn(\"region\",trim(col_trim_df[\"region\"]))\n",
    "\n",
    "#casting column data types\n",
    "b = trim_df.withColumn(\"amount\", regexp_replace(col(\"amount\"),\",\", \"\"))\\\n",
    "    .withColumn(\"order_id\",expr(\"try_cast(order_id as int)\"))\\\n",
    "    .withColumn(\"amount\",expr(\"try_cast(amount as double)\"))\\\n",
    "    .withColumn(\"amount\", when(col(\"amount\").isNotNull(),col(\"amount\")).otherwise(None))\n",
    "         #filling blank and currpted cell by string 'null', rekoving 'error' word\n",
    "b.printSchema()\n",
    "b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd0e0cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n",
      "+--------+-------------+-------+----------+-------+------+\n",
      "|order_id|customer_name|product|order_date| amount|region|\n",
      "+--------+-------------+-------+----------+-------+------+\n",
      "|       1|        Alice| Laptop|2023-01-15|55000.0| North|\n",
      "|       2|          Bob| Mobile|2023-02-15|30000.0| South|\n",
      "|       3|        Carol| Tablet|2023-03-10|   NULL|  East|\n",
      "|       4|        David| Laptop|2023-01-13|65000.0|  West|\n",
      "|       5|          Eve| Mobile|2023-04-05|   NULL| North|\n",
      "|       6|        Frank| Laptop|2023-05-05|72000.0| South|\n",
      "|       7|        Grace| Tablet|2023-06-18|48000.0|  East|\n",
      "|       7|        Grace| Tablet|2023-06-18|48000.0|  East|\n",
      "|       8|        Heidi| Mobile|      NULL|50000.0|  West|\n",
      "|       9|         Ivan| Laptop|2023-07-22|85000.0| North|\n",
      "|      10|         Judy| Tablet|2023-08-30|   NULL| South|\n",
      "+--------+-------------+-------+----------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#parsing date\n",
    "dfdate=b.withColumn(\"order_date\",\n",
    "                    coalesce(\n",
    "                        to_date(col(\"order_date\"),\"yyyy-MM-dd\"),\n",
    "                        to_date(col(\"order_date\"),\"yyyy-dd-MM\"),\n",
    "                        to_date(col(\"order_date\"),\"dd-MM-yyyy\"),\n",
    "                        to_date(col(\"order_date\"),\"dd/MM/yyyy\"),\n",
    "                        to_date(col(\"order_date\"),\"MM/dd/yyyy\"),\n",
    "                        to_date(col(\"order_date\"),\"yyyy/MM/dd\"),\n",
    "                        to_date(col(\"order_date\"),\"MMM dd, yyyy\")\n",
    "                    ))\n",
    "dfdate.printSchema()\n",
    "dfdate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfd1f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------+----------+-------+------+\n",
      "|order_id|customer_name|product|order_date| amount|region|\n",
      "+--------+-------------+-------+----------+-------+------+\n",
      "|       9|         Ivan| Laptop|2023-07-22|85000.0| North|\n",
      "|       2|          Bob| Mobile|2023-02-15|30000.0| South|\n",
      "|       7|        Grace| Tablet|2023-06-18|48000.0|  East|\n",
      "|       4|        David| Laptop|2023-01-13|65000.0|  West|\n",
      "|       6|        Frank| Laptop|2023-05-05|72000.0| South|\n",
      "|       1|        Alice| Laptop|2023-01-15|55000.0| North|\n",
      "|       8|        Heidi| Mobile|      NULL|50000.0|  West|\n",
      "|       3|        Carol| Tablet|2023-03-10|   NULL|  East|\n",
      "|       5|          Eve| Mobile|2023-04-05|   NULL| North|\n",
      "|      10|         Judy| Tablet|2023-08-30|   NULL| South|\n",
      "+--------+-------------+-------+----------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#remove duplicates\n",
    "dropdupdf=dfdate.dropDuplicates()\n",
    "dropdupdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58842f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null record df:-\n",
      "+--------+-------------+-------+----------+-------+------+\n",
      "|order_id|customer_name|product|order_date| amount|region|\n",
      "+--------+-------------+-------+----------+-------+------+\n",
      "|       3|        Carol| Tablet|2023-03-10|   NULL|  East|\n",
      "|       5|          Eve| Mobile|2023-04-05|   NULL| North|\n",
      "|       8|        Heidi| Mobile|      NULL|50000.0|  West|\n",
      "|      10|         Judy| Tablet|2023-08-30|   NULL| South|\n",
      "+--------+-------------+-------+----------+-------+------+\n",
      "\n",
      "Cleaned records df:-\n",
      "+--------+-------------+-------+----------+-------+------+\n",
      "|order_id|customer_name|product|order_date| amount|region|\n",
      "+--------+-------------+-------+----------+-------+------+\n",
      "|       1|        Alice| Laptop|2023-01-15|55000.0| North|\n",
      "|       2|          Bob| Mobile|2023-02-15|30000.0| South|\n",
      "|       4|        David| Laptop|2023-01-13|65000.0|  West|\n",
      "|       6|        Frank| Laptop|2023-05-05|72000.0| South|\n",
      "|       7|        Grace| Tablet|2023-06-18|48000.0|  East|\n",
      "|       9|         Ivan| Laptop|2023-07-22|85000.0| North|\n",
      "+--------+-------------+-------+----------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#drop null rows make seperate file\n",
    "#filter rows with nulls (or invalid values) into one DataFrame, and valid rows into another.\n",
    "\n",
    "#rows with nulls in critical fields\n",
    "error_df=dropdupdf.filter(\n",
    "    col(\"order_date\").isNull() | col(\"amount\").isNull())\\\n",
    "        .orderBy(\"order_id\")\n",
    "\n",
    "#clean rows (no nulls in critical fields)\n",
    "clean_df=dropdupdf.filter(\n",
    "    col(\"order_date\").isNotNull() & col(\"amount\").isNotNull())\\\n",
    "        .orderBy(\"order_id\")\n",
    "\n",
    "#printiting error_df and clean_df\n",
    "print(\"Null record df:-\")\n",
    "error_df.show()\n",
    "print(\"Cleaned records df:-\")\n",
    "clean_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60d32eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of cleaned data:-\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#clean df schema\n",
    "print(\"Schema of cleaned data:-\")\n",
    "clean_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d03f417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales per day:-\n",
      "+----------+-----------+\n",
      "|order_date|total_sales|\n",
      "+----------+-----------+\n",
      "|2023-01-13|    65000.0|\n",
      "|2023-01-15|    55000.0|\n",
      "|2023-02-15|    30000.0|\n",
      "|2023-05-05|    72000.0|\n",
      "|2023-06-18|    48000.0|\n",
      "|2023-07-22|    85000.0|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#total sales per day.\n",
    "#group by order_date and calculate total sales\n",
    "sales_per_day=clean_df.groupBy(\"order_date\")\\\n",
    "    .agg(sum(col(\"amount\")).alias(\"total_sales\"))\\\n",
    "        .orderBy(\"order_date\")\n",
    "print(\"Sales per day:-\")\n",
    "sales_per_day.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8d2666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top selling products:-\n",
      "+-------+-----------+\n",
      "|product|top_product|\n",
      "+-------+-----------+\n",
      "| Laptop|   277000.0|\n",
      "| Tablet|    48000.0|\n",
      "| Mobile|    30000.0|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#top 5 product (by revenue)\n",
    "top_product_df=clean_df.groupBy(\"product\")\\\n",
    "    .agg(sum(col(\"amount\")).alias(\"top_product\"))\\\n",
    "        .orderBy(col(\"top_product\").desc())\\\n",
    "            .limit(5)\n",
    "print(\"Top selling products:-\")\n",
    "top_product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec307ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly revenue summary:-\n",
      "+----------+-----------+\n",
      "|year_month|sum_revenue|\n",
      "+----------+-----------+\n",
      "|    2023-1|   120000.0|\n",
      "|    2023-2|    30000.0|\n",
      "|    2023-5|    72000.0|\n",
      "|    2023-6|    48000.0|\n",
      "|    2023-7|    85000.0|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#monthly revenue\n",
    "#extract year-month \n",
    "year_month_df=clean_df.withColumn(\"year_month\",concat_ws(\"-\", year(col(\"order_date\")),month(col(\"order_date\"))))\n",
    "\n",
    "#group by year_month and sum_revenue\n",
    "sum_revenue_df=year_month_df.groupBy(\"year_month\")\\\n",
    "    .agg(sum(\"amount\").alias(\"sum_revenue\"))\\\n",
    "        .orderBy(\"year_month\")\n",
    "print(\"Monthly revenue summary:-\")\n",
    "sum_revenue_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2544eeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----+\n",
      "|region|product|total_sale|rank|\n",
      "+------+-------+----------+----+\n",
      "|  East| Tablet|   48000.0|   1|\n",
      "| North| Laptop|  140000.0|   1|\n",
      "| South| Laptop|   72000.0|   1|\n",
      "|  West| Laptop|   65000.0|   1|\n",
      "+------+-------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "#RANK PRODUCTS (WINDOW FUNCTION)\n",
    "#region + product level revenue\n",
    "region_product_group_df=clean_df.groupBy(\"region\",\"product\")\\\n",
    "    .agg(sum(col(\"amount\")).alias(\"total_sale\"))\n",
    "\n",
    "#define windows function\n",
    "windowspec=Window.partitionBy(\"region\").orderBy(col(\"total_sale\").desc())\n",
    "\n",
    "#appy row_number\n",
    "ranked=region_product_group_df.withColumn(\"rank\",row_number().over(windowspec))\n",
    "\n",
    "#filter top product per region\n",
    "top_product_per_region_df=ranked.filter(col(\"rank\")==1)\n",
    "\n",
    "#show result\n",
    "top_product_per_region_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b21e160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export and delivery\n",
    "#save clean_df in outpute folder\n",
    "clean_df.coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "        .option(\"header\",\"true\")\\\n",
    "            .csv(\"../output/cleaned_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099ef327",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save report tables (csv) in output folder\n",
    "#daily sale report\n",
    "sales_per_day.coalesce(1)\\\n",
    "    .write\\\n",
    "        .mode(\"overwrite\")\\\n",
    "            .option(\"header\",\"true\")\\\n",
    "                .csv(\"../output/reports/daily_sales/\")\n",
    "\n",
    "#monthly sale reports                \n",
    "sum_revenue_df.coalesce(1)\\\n",
    "    .write\\\n",
    "        .mode(\"overwrite\")\\\n",
    "            .option(\"header\",\"true\")\\\n",
    "                .csv(\"../output/reports/monthly_revenue/\")\n",
    "\n",
    "#top product sale report region wise\n",
    "top_product_per_region_df.coalesce(1)\\\n",
    "    .write\\\n",
    "        .mode(\"overwrite\")\\\n",
    "            .option(\"header\",\"true\")\\\n",
    "                .csv(\"../output/reports/top_product_per_region/\")\n",
    "\n",
    "#top product \n",
    "top_product_df.coalesce(1)\\\n",
    "    .write\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .option(\"header\",\"true\")\\\n",
    "            .csv(\"../output/reports/top_products/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e0d95bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving clean data in parquet file\n",
    "clean_df.write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "        .parquet(\"../output/clean_data_in_parquet/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
